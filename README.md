# GPT-2-Tutorials
```markdown
Q: 这是什么？
A: 这是一份[GPT-2的学习代码](https://github.com/wzy6642/GPT-2-Tutorials/blob/main/GPT_2%20Tutorials.ipynb)，通过这份代码，你可以了解GPT模型的构建方法以及训练和推理的过程，我们以加法运算和文本生成为例给出了GPT-2的运行结果。
Q: 我需要配置很复杂的运行环境吗？
A: 完全不需要哦，只需要PyTorch版本号1.6.0就可以，CPU也可以运行哈。同时我们在代码中给出了Colab链接，你可以借助Colab的GPU资源更快地训练模型。
Q: 我需要下载checkpoints吗？
A: 完全不需要，我们的模型是从头开始训练，这样可以帮助大家更好地了解GPT-2完整的运行方式。
```
# Package
PyTorch == 1.6.0
# Reference
* [minGPT](https://github.com/karpathy/minGPT)
* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
